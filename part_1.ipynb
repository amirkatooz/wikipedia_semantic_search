{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "\n",
    "In this part, I downloaded a bunch of wikipedia articles, cleaned their contents, and stored them in postgreSQL tables. There are three mini-libraries that I used frequently in this part:\n",
    "- download_from_wikipedia\n",
    "- cleaner\n",
    "- database_manager\n",
    "\n",
    "First step is to select a category to download the corresponding articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "category = 'machine learning'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading articles\n",
    "\n",
    "Now we should use the `get_articles` function from `download_from_wikipedia` library to download the articles under `machine learning`. Every category has some sub-categories so we can also download articles under the sub-cats of machine learning. We can determine how deep the sub-category level can go.\n",
    "\n",
    "I want to download around 1000 articles under machine learning so I have to find the proper `sub_cat_level`. **After passing the category and sub-category depth to the `get_articles` function, it will tell us how many articles are there and asks whether we want to continue to download or change the depth.**\n",
    "\n",
    "since downloading 1000 articles takes some time, my function reports its progress after downloading every 200 articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.download_from_wikipedia import get_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1082 articles under category \"machine learning\" (subcategory depth = 2)\n",
      "Enter \"y\" to download all articles. Otherwise, enter \"n\" and change the category and/or subcategory depth.\n",
      "y\n",
      "downloaded 200 articles so far...\n",
      "downloaded 400 articles so far...\n",
      "downloaded 600 articles so far...\n",
      "downloaded 800 articles so far...\n",
      "downloaded 1000 articles so far...\n",
      "1082 articles downloaded successfully!\n"
     ]
    }
   ],
   "source": [
    "wiki_df = get_articles(category, sub_cat_level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1082, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleaning\n",
    "\n",
    "Now that we have a dataframe of all machine learning article contents, we need to clean their texts. I used the `cleaner` library for this purpose.\n",
    "\n",
    "Some of the wikipedia pages don't have any contents and some of them are User/Template/File pages, so first we have to get rid of those articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1074, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib.cleaner import df_cleaner, text_cleaner, title_cleaner\n",
    "wiki_df = df_cleaner(wiki_df)\n",
    "wiki_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, we will lemmatize the contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_content_list = [' '.join([word.lemma_ for word in nlp(content)]) for content in wiki_df['content'].tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we call the `text_cleaner` function to clean the lemmatized contents. I've used RegExr to get rid of formulas (Latex), white spaces, numbers, and none-letters. We also need to clean the titles before writing them to postgres. Postgres doesn't like single quotes. Many of the titles do have a single quote so in order to put them in postgres we need to add another single quote to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_content_list = [text_cleaner(content) for content in lemmatized_content_list]\n",
    "\n",
    "wiki_df['clean_content'] = clean_content_list\n",
    "wiki_df['title'] = wiki_df['title'].apply(title_cleaner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing to database\n",
    "\n",
    "I used my `database_manager` library in order to create a schema with three tables and store the wikipedia articles in three tables within that database. The tables are:\n",
    "- **`articles`**: Primary key is page ID. Includes title and cleaned content for each page ID.\n",
    "- **`categories`**: Primary key is category ID. Includes category names (or titles).\n",
    "- **`article_category`**: Primary key is page (article) ID. Includes category ID for each articled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lib.database_manager import create_schema, query_to_dataframe, insert_to_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema already exists!\n"
     ]
    }
   ],
   "source": [
    "create_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to `articles` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_id = query_to_dataframe('SELECT article_id FROM articles')\n",
    "\n",
    "if len(articles_id) > 0:\n",
    "    wiki_df = wiki_df[wiki_df['pageid'].apply(lambda p_id: p_id not in articles_id['article_id'].tolist())]\n",
    "    \n",
    "articles_to_insert = ', '.join([\"({}, '{}', '{}')\".format(r['pageid'], r['title'], r['clean_content'])\\\n",
    "                       for i, r in wiki_df.iterrows()])\n",
    "\n",
    "if articles_to_insert != '':\n",
    "    insert_to_db('INSERT INTO articles VALUES {}'.format(articles_to_insert))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to `categories` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "\n",
    "category_id_to_insert = wikipedia.WikipediaPage(category).pageid\n",
    "category_title_to_insert = title_cleaner(category)\n",
    "\n",
    "try:\n",
    "    insert_to_db(\"INSERT INTO categories VALUES ({}, '{}')\".format(category_id_to_insert, category_title_to_insert))\n",
    "except:\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to `article_category` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articles_cats_to_insert = ', '.join([\"({}, {})\".format(r['pageid'], category_id_to_insert)\\\n",
    "                                     for i, r in wiki_df.iterrows()])\n",
    "\n",
    "if articles_cats_to_insert != '':\n",
    "    insert_to_db('INSERT INTO article_category VALUES {}'.format(articles_cats_to_insert))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
